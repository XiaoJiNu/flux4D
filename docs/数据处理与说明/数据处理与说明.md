# PandaSet 数据处理与说明

## 数据位置与目录结构

- 本机数据根目录：`/home/yr/yr/data/automonous/pandaset`
- 每个场景是一个数字目录，例如 `001/`、`002/`。
- 关键子目录（以 `001` 为例）：
  - `001/lidar/`：点云帧（`00.pkl.gz`...）、`timestamps.json`、`poses.json`
  - `001/camera/<camera_name>/`：图像帧（`00.jpg`...）、`timestamps.json`、
    `poses.json`、`intrinsics.json`

注意：仓库内只保存索引文件，原始数据仍在上述外部路径，不复制进仓库。

## 索引文件（PKL）

阶段1生成两个索引文件：

- `data/metadata/pandaset_full_clips.pkl`
- `data/metadata/pandaset_tiny_clips.pkl`

说明：索引文件为本地生成产物，默认被 `.gitignore` 忽略，需要在本机重新生成。

PKL 结构为一个字典：

- `meta`：索引级元信息
  - `data_root`：PandaSet 根目录
  - `preset`：索引预设（`paper`/`debug`）
  - `target_fps`：切片目标帧率（默认 10Hz）
  - `scene_ids`：参与构建的场景列表
  - `test_scenes`：测试场景列表（`paper` 预设默认使用论文固定 test logs）
  - `camera_names`：保留的相机列表（`paper` 预设默认仅保留 `front_camera`）
  - `settings`：切片设置列表（每个 setting 描述片段长度/步长/输入帧/目标帧等）
  - `total_clips`：clip 数量
  - `created_at`：生成时间（UTC）
- `clips`：clip 记录列表
  - 每条记录包含：
    - `clip_id` / `scene_id` / `split`
    - `fps` / `fps_actual`
    - `setting`：切片设置名（如 `train_interpolation` / `train_future` / `eval_future`）
    - `clip_len_frames` / `stride_frames`
    - `input_frame_indices` / `target_frame_indices`（clip 内相对帧索引）
    - `input_frame_ids` / `target_frame_ids`（scene 内绝对帧号）
    - `frame_start` / `frame_end` / `frame_ids`
    - `timestamps`：`lidar` 与 `camera`（逐帧）
    - `intrinsics`：每个相机的内参
    - `extrinsics`：`lidar` 与 `camera` 的 `poses.json`（位置 + 四元数）
    - `image_paths` / `lidar_paths`：相对 `data_root` 的路径
    - `views`：可见相机名称列表

## 生成命令

```bash
python scripts/preprocess_flux4d.py \
  --data-root /home/yr/yr/data/automonous/pandaset \
  --out-full data/metadata/pandaset_full_clips.pkl \
  --out-tiny data/metadata/pandaset_tiny_clips.pkl
```

常用可选项：

- `--preset paper`（默认）：按论文/补充材料切片与 split（11/16 帧、5 帧 overlap、每 20 帧采样评测 snippet、固定 test logs，仅 `front_camera`）
- `--preset debug --clip-len-s 1.5 --stride-s 1.5`（旧的秒级切片方式，用于快速调试）
- `--tiny-scenes 001,002` 或 `--tiny-num-scenes 2`
- `--val-scenes 001,011,...`（手动指定测试集 scenes）
- `--val-num-scenes 10`（仅 debug preset 使用）
- `--target-fps 10`（目标帧率，设为 0 则使用实际推断）
- `--camera-names front_camera`（覆盖保留的相机列表）

Makefile 入口：

```bash
make pandaset-index
```

## 数据使用示例

```python
from flux4d.datasets.pandaset_clips import load_clip_index, load_clip

payload = load_clip_index("data/metadata/pandaset_tiny_clips.pkl")
first_clip = payload["clips"][0]
clip = load_clip(first_clip, data_root="/home/yr/yr/data/automonous/pandaset")

print(clip["clip_id"], clip["views"])
print("lidar[0]", clip["lidar_paths"][0])
print("front[0]", clip["image_paths"][clip["views"][0]][0])
```

## PKL 快速检查脚本

可以用脚本快速查看索引概要或进入断点：

```bash
python scripts/inspect_pkl.py --path data/metadata/pandaset_tiny_clips.pkl
python scripts/inspect_pkl.py --path data/metadata/pandaset_tiny_clips.pkl --breakpoint
```

断点中可直接使用 `payload`（全量）和 `clip`（当前 clip）变量。

## 训练/验证拆分说明

- `paper` 预设默认按补充材料 C.1 的固定 test logs：
  `001,011,016,065,084,090,106,115,123,158`；其余为 train。
- 如需覆盖，可显式传入 `--val-scenes`。
- `debug` 预设保留旧行为：默认按场景 ID 排序后取最后 `--val-num-scenes` 个作为测试集。

## 时间戳与时间建模（以 LiDAR 为时间轴）

PandaSet 的各相机 `timestamps.json` 与 LiDAR `timestamps.json` 往往存在**稳定但相机间不同的固定 offset**。
本项目的索引构建保持“按帧号（index）切片”的简单假设：即在同一 `frame_index` 下读取对应的
LiDAR 帧与相机帧（符合论文在 PandaSet 上使用单相机 + 10Hz 的常见用法）。

补充材料 A.2 要求：每个 Gaussian 继承 LiDAR timestamp，并 **min-max 归一化到 [0,1]**。
该做法也能避免直接使用 epoch 秒写入 `float32` 的精度问题（否则 10Hz 的 0.1s 间隔可能被抹平）。

因此，本项目在 Lift 初始化阶段使用 **clip 内 min-max 归一化时间**：

- 对当前 clip（被选中的帧集合），取 `t_min = min(timestamps.lidar)`，`t_max = max(timestamps.lidar)`。
- 每个帧的时间使用 `t_norm[k] = (t[k] - t_min) / (t_max - t_min)`（范围 0~1）。
- `t_norm` 写入高斯的 `timestamps`（`float32`）用于后续时间推进。

对应实现：

- `src/flux4d/lift/lift_lidar.py`（`build_initial_gaussians_for_clip`）

## 数据加载与坐标系约定（对齐 pandaset-devkit）

本项目的数据读取与几何处理参考 `pandaset-devkit` 的实现与约定（路径：
`/home/yr/yr/code/cv/AutoLabel/SSL/pandaset-devkit`），核心点如下：

- **点云坐标系**：PandaSet 的 LiDAR 点云默认存储在 **world** 坐标系中（devkit 文档说明：
  静态物体在不同帧坐标保持不变）。因此：
  - 投影到相机时，直接使用 **world->camera** 外参变换即可。
  - 若需要 ego/LiDAR 坐标系下的点（便于做 BEV 或与车辆坐标对齐），应使用
    **world->ego = inv(ego->world)** 的变换。
- **相机位姿**：`camera/<name>/poses.json` 提供的是相机从 sensor->world 的位姿（位置 + 四元数）。
- **LiDAR 位姿**：`lidar/poses.json` 也提供 sensor->world 的位姿；但当点云已在 world 坐标系时，
  不应再把 `lidar_pose` 作为 lidar->world 额外乘一次。

对应实现：

- 点云读取：`src/flux4d/lift/lift_lidar.py:69`（`load_lidar_frame`）
  - 通过 `pandas.read_pickle` 读取 `*.pkl.gz`。
  - 可选 `sensor_id` 用于筛选 `d` 字段（`-1`=全部，`0/1`=单 LiDAR）。
- 图像读取：`src/flux4d/lift/lift_lidar.py:116`（`load_image_rgb`）
  - 使用 Pillow 读取，并通过 `copy()+close()` 避免句柄问题。
- world->camera：`src/flux4d/lift/lift_lidar.py:233`（`transform_lidar_to_camera(points_in_world=True)`）
- world->ego：`src/flux4d/lift/lift_lidar.py:265`（`transform_world_to_ego`）

## 点云处理（下采样、尺度估计、采色）

- **体素下采样**：`src/flux4d/lift/lift_lidar.py:283`（`voxel_downsample_points`）
  - 将点按 `floor(p / voxel_size)` 分桶，每个体素用均值点代表，便于稳定统计与后续 Lift。
- **尺度估计（kNN）**：`src/flux4d/lift/lift_lidar.py:324`（`compute_knn_mean_distance`）
  - 优先使用 `scipy.spatial.cKDTree`，缺失时小点数退化为暴力计算。
  - 补充材料 A.2 使用 **3-NN 平均距离** 初始化尺度，并对尺度做对数变换（scale 参数进入网络）。
- **投影采色**：`src/flux4d/lift/lift_lidar.py:569`（`colorize_points_multi_view`）
  - 将点投影到相机像素坐标后，从同步图像中采样 RGB 作为点颜色；多视角按顺序“先命中先填充”。

## 可视化门禁（原始点云 + 投影到图像）

可视化脚本：`tools/vis/vis_lift_alignment.py`。该脚本输出两类结果：

1) **点云投影到图像**（raw 与 downsampled/lift 两套）：
   - `rgb.png`：原图
   - `rgb_points_raw.png` / `rgb_points_lift.png`：点云投影叠加
   - `depth_gray_raw.png` / `depth_gray_lift.png`：稀疏深度图（最近点写入像素）

2) **原始点云可视化（BEV）**（可选，world/ego 两套；用 matplotlib colormap + Pillow 输出 PNG，
   不依赖 OpenGL/可视化窗口）：
   - `bev_world_raw.png` / `bev_world_lift.png`
   - `bev_ego_raw.png` / `bev_ego_lift.png`

推荐在 `gaussianstorm` 环境运行（系统 Python 可能存在 numpy/pandas 二进制不兼容）：

```bash
/home/yr/anaconda3/envs/gaussianstorm/bin/python tools/vis/vis_lift_alignment.py \
  --index-path data/metadata/pandaset_tiny_clips.pkl \
  --clip-index 0 --frame-index 0 --camera front_camera \
  --output-dir assets/vis/lift_alignment_devkit \
  --voxel-size 0.2 --max-points-raw 80000 --max-points 80000 \
  --point-color-mode depth --overlay-base gray \
  --save-pointcloud --bev-x-range -50 50 --bev-y-range -50 50 --bev-size 800 800
```

常用参数：

- `--lidar-sensor-id -1/0/1`：选择全部点或单 LiDAR（对应 `d` 字段）。
- `--point-color-mode depth/from_image/fixed`：点颜色来源（深度伪彩/图像采样/固定色）。
- `--voxel-size`：体素下采样尺寸（米）。
- `--save-pointcloud`：额外保存 BEV（world/ego）点云可视化。
