# 任务与约束复述

## 目标

- 在 PandaSet 上复现 Flux4D 的主要指标（PSNR/SSIM/Depth RMSE，以及 Scene Flow：EPE3D/Acc5/Acc10/θϵ/EPE-3way/分桶 EPE），并输出可视化结果。
- 实现无监督 4D 重建：预测 3D 高斯及其运动，支持任意时间与视角渲染。

> 注：论文中的 “flow” 指标口径为 **Scene Flow**（LiDAR 点上的 3D flow），主要报告 EPE3D/Acc5/Acc10/θϵ/EPE-3way/分桶 EPE；
> 图像平面渲染的 flow 主要用于可视化与动态重加权，而不是用 2D Flow EPE 作为主要指标。

## 约束与资源

- 数据集：PandaSet（/home/yr/yr/data/automonous/pandaset），后续可扩展 Waymo。
- 参考代码：GaussianSTORM（/home/yr/yr/code/cv/AutoLabel/SSL/GaussianSTORM_all/GaussianSTORM，环境 storm，环境路径为/home/yr/anaconda3/envs/gaussianstorm）。
- 运行环境路径为/home/yr/anaconda3/envs/gaussianstorm
- 硬件：1×5090 24G，需要控制视图数、分辨率与高斯数量。
  备注：我在24G的5090上是进行调试,可以将batch调小,所以直接和论文中的视图数与分辨率保持一致即可,我最终会在A100上训练.
- 无官方仓库，需按论文与翻译落地实现。
- 代码开发要遵守google python代码开发规范，并且每个类和函数需要有注释，至少要说明输入输出参数说明以及实现的功能和实现方法是什么
- 每个阶段完成后，要用git进行版本管理，方便修改与记录。

# 论文技术拆解

## 任务定义与场景表示

- 输入：RGB（论文 PandaSet 默认使用 `front_camera`；可扩展多相机但不作为对齐默认）、360° LiDAR 点云、内外参与时间戳。
- 输出：一组 3D 高斯 g_i = (p_i, s_i, R_i, c_i, α_i) + 速度 v_i + 时间戳 t_i。
- 目标：在连续时间上推进高斯并可微渲染，支持新视角/新时间的 RGB、Depth、Flow。

## 初始化（Lift）

- 从每个 LiDAR 帧 P_k 初始化高斯位置 p_i。
- 缩放 s_i：取 3-NN 平均距离并做 `log + softplus` 稳定训练（补充材料 A.2）。
- 颜色 c_i：将点投影到同步相机图像 I_k 采样。
- 旋转 R_i：随机初始化四元数并归一化（补充材料 A.2）。
- 不透明度 α_i：初始化为 0.5（补充材料 A.2）。
- 时间戳 t_i：继承 LiDAR timestamp，并对每个 clip 做 min-max 归一化到 `[0,1]`（补充材料 A.2）；速度初值 v_i = 0。
- 聚合多帧高斯得到 G_init。
- 无界场景：按补充材料 A.2 计算 AABB 后在上半球采样 1,000,000 个 sky 点（半径 `4 * AABB_length`），并在 3D 球体内采样随机点增强鲁棒性。

> 注：本复现实测 PandaSet 的 `.pkl.gz` 点云坐标默认已在 `world/log frame` 下（参考 pandaset-devkit 的数据定义），
> 因此初始化时直接使用 `p_world`；不额外做 `sensor/ego → world` 的坐标转换。若后续切换数据源，需要在索引中明确点云坐标系口径。

## 预测网络 fθ（稀疏 3D U-Net）

- 采用带稀疏卷积的 3D U-Net 作为 fθ（补充材料：SparseUNet adopted from torchsparse）。
- 输入：G_init 与时间戳 T（体素化后形成稀疏张量）。
- 输出：精炼高斯参数 G 与速度 V（逐高斯预测）。
- 多场景联合训练形成隐式先验，自动分解静态/动态。

## 时间推进与渲染

- 线性运动模型：
  p_i(t') = p_i(t_i) + v_i * (t' - t_i)
- 使用 3DGS 可微渲染生成 RGB/Depth。
- 在图像平面渲染 motion flow（rendered flow），用于可视化与动态重加权；Scene Flow 评测在 LiDAR 点上进行（论文 4.1）。

## 损失函数（Flux4D-base）

- 总损失：
  L = L_recon + λ_vel L_vel
- 重建损失：
  L_recon = λ_rgb L1(RGB) + λ_SSIM SSIM + λ_depth L1(Depth)
- 速度正则：
  L_vel = (1/M) Σ_i ||v_i||_2
- 论文权重参考：λ_rgb=0.8，λ_SSIM=0.2，λ_depth=0.01，λ_vel=5e-3。

## 迭代优化与运动增强（Flux4D）

- 迭代优化：渲染后对高斯求 3D 梯度，将 (G, grad) 输入 f_φ 进行 1–2 次细化，提升细节一致性。
- 运动增强：引入多项式运动参数化更好拟合加速/转弯等复杂动力学。
- 动态重加权：根据渲染光流对光度损失逐像素加权，强调动态区域。

## 与 GaussianSTORM 的对应与改造

- 可复用：gsplat/3DGS 渲染器、数据管线、多视图采样与训练脚手架。
- 关键改造：
  - 表示层：从 STORM 的 token/特征驱动改为 LiDAR 初始化高斯 + 速度预测。
  - 主干网络：以稀疏 3D U-Net 为主，输出逐高斯更新与速度。
  - 损失：以光度+深度+速度正则为核心，加入动态重加权与迭代 refinement。
  - 训练：多场景联合训练，支持高分辨率多视图输入。

## 对原提示词的修正/补充

- 预测模块主干为稀疏 3D U-Net，而非 Transformer。
- 论文区分 Flux4D-base 与 Flux4D（包含迭代优化与运动增强）。
- 无界场景处理加入随机远景/天空点，这一实现细节需补充。

# 分阶段实现路线图

## 阶段0：环境与基线验证

- 在 storm 环境跑通 GaussianSTORM demo，确认渲染与数据读取链路。
- 核对 PandaSet 时间同步与标定文件可用性。
- 24G 5090 调试保持论文视图数与分辨率，仅通过 batch/梯度累积控显存。

## 阶段1：PandaSet 数据索引与切片

- 生成 `data/metadata/pandaset_full_clips.pkl` 与 `data/metadata/pandaset_tiny_clips.pkl`。我想用Pkl存储数据索引。
- 每条记录包含时间戳、相机/雷达位姿（pose）、图像/点云路径与可见视图。
- **强制写入坐标系口径（本复现约定）**：
  - `lidar_points_frame = world`：PandaSet 点云默认在 `world/log frame`。
  - `pose_convention = sensor_to_world`：索引中的 `poses.json` 按 `sensor→world` 解释（与本仓库 `pose_to_matrix` 一致）。
  - `ego0_frame_index = 0`：定义 `ego0` 为 snippet 的第 0 帧自车系（用于网络内部体素化）。
- 对齐论文/补充材料设置（强制写进索引与文档）：
  - PandaSet split（补充材料 C.1）：test logs = `001,011,016,065,084,090,106,115,123,158`；其余为 train。
  - 训练切分（补充材料 C.2）：
    - Interpolation：1s snippet（11 帧，10Hz），相邻 snippet 5 帧 overlap；输入帧 `[0,2,4,6,8,10]`，目标帧 `[1,3,5,7,9]`。
    - Future prediction：1.5s snippet（16 帧，10Hz），相邻 snippet 5 帧 overlap；输入帧 `[0,2,4,6,8,10]`，目标帧 `[1,3,5,7,9,11,12,13,14,15]`。
  - 评测采样（正文 4.1）：每 20 帧采样一个 snippet，每 log 产生 4 个不重叠评测 snippet。
- 对于生成的数据索引和pandaset数据本身，以及数据使用方法，要有说明文档，生成一个数据说明文档放在docs/数据处理与说明/数据处理与说明.md中

## 阶段2：LiDAR → 高斯（Lift）实现与门禁

- 实现体素下采样、3-NN 平均距离尺度估计（`log + softplus`）、随机四元数初始化、图像投影采色（默认 `front_camera`）。
- 生成 `G_init_world` 并统计高斯数量与尺度分布（点云在 `world` 下直接初始化并跨帧聚合）。
- 为阶段3准备 `world→ego0` 的坐标变换定义：仅在送入网络前将 `p_world/q_world` 转到 `ego0`。
- 可视化门禁：渲染 RGB/Depth 与原图/投影深度对比对齐。

## 阶段3：稀疏 3D U-Net 预测器（Flux4D-base）

- 体素化 `G_init_ego0`（由 `G_init_world` 通过 `world→ego0` 得到），构建稀疏张量输入 3D U-Net。
- 输出逐高斯属性更新 `ΔG_ego0` 与速度 `V_ego0`，并在模型输出阶段将速度旋转到 `V_world`（用于速度正则、时间推进与渲染）。
- tiny clip 过拟合验证：重建 loss 下降、PSNR/SSIM 上升、速度趋稳。
- 门禁本地回归产物：`assets/vis/stage3_overfit_run2/`（含 `ckpt_last.pt`；目录被 `.gitignore` 忽略）。

## 阶段4：时间推进与光流渲染

- 实现线性运动推进与多时间步渲染。
- 渲染 RGB/Depth/Rendered Flow（图像平面），检查 flow 方向与相邻帧位移一致性。
- 建立插值/未来帧渲染流程，贴近论文评测设置。

## 阶段5：Flux4D 完整增强

- 加入迭代优化：渲染后获取 3D 梯度，进行二次 refinement。
- 加入动态重加权：由渲染 flow 构造权重图，强化动态区域损失。
- 运动增强：按补充材料采用常加速度（`ℓ=1`，`vdim=6`）等多项式运动参数化；做消融确认收益后启用。

## 阶段6：全量训练与评测

- PandaSet full 训练，输出 PSNR/SSIM/Depth RMSE，以及 Scene Flow 指标（EPE3D/Acc5/Acc10/θϵ/EPE-3way/分桶 EPE）。
- 评测设置对齐论文：输入 {0,2,4,6,8,10}，评估插值与未来帧。
- 生成典型场景 4D 重建视频与新视角对比。

# 各阶段详细说明

## 阶段3：稀疏 3D U-Net 预测器（Flux4D-base）—实现方案（对齐正文 + 补充材料 A.2/A.3）

> 本阶段所有可调参数统一写入单文件配置：`configs/flux4d.py`（按 data/init/coord/voxel/model/render/loss/train 分组）。
> 论文/补充材料未显式规定训练的 canonical 坐标系。本复现采用**混合坐标系策略（实现选择）**：
>
> - 主存储/渲染：`world/log frame`（点云与高斯中心在 world；渲染使用 `T_cam_world`）。
> - 网络体素化：`ego0`（snippet 第 0 帧自车系），用于稳定 `point_cloud_range` 与稀疏卷积的计算范围。
> - 速度语义：输出阶段将 `V_ego0` 旋转为 `V_world`，以便 `L_vel` 与时间推进在 world 下保持“静态背景≈0”的语义。

### 3.1 目标与门禁（对齐补充材料）

**Flux4D-base 的网络定义（补充材料 Algorithm 1 / A.2）**：

- 输入：
  - `G_init ∈ R^{N×14}`：按顺序拼接 `[p(3), q(4), s(3), o(1), c(3)]`
  - `T ∈ R^{N×1}`：每个高斯的 capture time，**归一化到 [0,1]**
- 输出：
  - `ΔG ∈ R^{N×14}`：逐高斯残差（residue）
  - `V ∈ R^{N×vdim}`：逐高斯速度；`vdim = 3(ℓ+1)`，Flux4D-base 取 `ℓ=0` → `vdim=3`
- 更新：
  - `G = G_init + ΔG`
  - `q = normalize(q)`（补充材料采用 `q_init + Δq` 后归一化）

**门禁（tiny clip overfit）**：

- 重建 loss 持续下降、PSNR/SSIM 上升；
- `λ_vel=5e-3` 下 `||v||` 不发散且趋稳；
- 补充材料指出：对 1.5s snippet，训练 5,000 iter 易出现颜色伪影，建议至少 10,000 iter（本阶段 overfit 默认按 10,000）。

### 3.2 初始化（对齐补充材料 A.2）

- Position：每个高斯中心初始化为对应 LiDAR 点坐标 `p`
- Color：将该帧 LiDAR 点投影到图像采样颜色 `c`
- Scale：取 3-NN 平均距离作为尺度，随后做 `log + softplus` 稳定训练（具体开关/超参见 `configs/flux4d.py`）
- Orientation：随机初始化四元数并归一化
- Opacity：初始化为 0.5
- Time：继承 LiDAR timestamp，并 **min-max 归一化到 [0,1]**

**天空/远景建模（补充材料 A.2）**：

- 计算初始化高斯的 AABB（axis-aligned bounding box）
- 在以 AABB 中心为球心、半径为 `4 * AABB_length` 的上半球随机采样 **1,000,000** 个 sky 点并并入 `G_init`

### 3.3 坐标系规范（world 渲染 + ego0 体素化，强制）

**定义：**

- `world/log frame`：PandaSet log 的全局坐标系（点云默认在此坐标系下）。
- `ego0`：snippet 的第 0 帧 LiDAR/ego 坐标系（本复现用于网络体素化的参考系）。
- `T_world_ego0`：由 `ego0` 帧的 LiDAR pose 构建的刚体变换（`sensor→world`）。
- `T_ego0_world = inv(T_world_ego0)`。
- `R_world_ego0` 为 `T_world_ego0` 的旋转部分（3×3），`R_ego0_world = R_world_ego0^T`。

**训练前的坐标准备：**

- 初始化与聚合：得到 `G_init_world`（在 `world` 下）。
- 网络输入：将 `G_init_world` 转到 `ego0` 得到 `G_init_ego0`，用于体素化与稀疏 3D U-Net：
  - 点：`p_ego0 = T_ego0_world ⊗ p_world`
  - 旋转（建议做以避免特征混系）：`R_gauss_ego0 = R_ego0_world · R_gauss_world`

**网络输出与对外语义：**

- 网络在 `ego0` 下输出 `ΔG_ego0` 与 `V_ego0`。
- 对外输出速度使用 `world` 语义：
  - `V_world = R_world_ego0 · V_ego0`（速度为向量，仅旋转，不加平移；范数保持不变）
- 用于渲染/时间推进时，需要将更新后的中心回到 `world`：
  - `p_world = T_world_ego0 ⊗ p_ego0`（对更新后的 `p_ego0` 应用）

> 说明：该策略是工程实现选择，用于解决 world 下坐标漂移导致 `point_cloud_range` 不稳定的问题；两份 PDF 未强制规定 canonical frame。

### 3.4 体素化与稀疏张量（spconv）

补充材料给出 `G_init` 的特征维度，但未指定体素网格范围；本复现将以下参数全部置于配置文件：

- `point_cloud_range`（米）：`[x_min, y_min, z_min, x_max, y_max, z_max]`
- `voxel_size`（米）：`[vx, vy, vz]`
- `voxel_shape`：由以上两者派生计算（在 `configs/flux4d.py` 中自动计算）

稀疏张量构建：

- 稀疏坐标 `coord = floor((p_ego0 - pc_min) / voxel_size)`（在 ego0 系，pc_min 由 `point_cloud_range` 定义）
- 稀疏特征 `feat = concat([G_init(14), T(1)], dim=-1)` → `N×15`
- 同体素内点做 `mean pooling`，并记录 `point2voxel` 以支持 voxel→point 回填

### 3.5 网络结构（对齐补充材料 Fig. A1）

Flux4D-base 使用稀疏卷积 3D U-Net（补充材料：SparseUNet adopted from torchsparse）：

- 每个 block：`Conv3D → BatchNorm1d → LeakyReLU`
- U-Net 编解码结构，skip 连接采用 channel-wise concat
- 输出回到点级后，分别预测：
  - `ΔG (14)`
  - `V (vdim)`（base 为 3）

工程实现：补充材料使用 torchsparse；本复现按阶段3要求使用 `spconv` 实现等价稀疏 3D U-Net，结构/输入输出维度严格保持一致。

### 3.6 渲染与参数激活（对齐补充材料 A.2）

在渲染目标时间 `t'` 时：

1) 残差更新（ego0）：`G_ego0 = G_init_ego0 + ΔG_ego0`
2) 输出速度转 world：`v_world = R_world_ego0 · v_ego0`
3) 将中心转回 world 并做时间推进（常速度，`ℓ=0`）：`p_world(t') = p_world + v_world * (t' - t_i)`
4) 激活与稳定化（渲染前）：
   - `opacity = sigmoid(opacity)`
   - `color = sigmoid(color)`
   - `scale clamp to [0m, 1m]`
5) 使用 `gsplat` 渲染（RGB+Depth），并用正文权重计算 `L_recon`，叠加 `λ_vel L_vel`（本复现建议 `L_vel` 在 `v_world` 上计算）

## 阶段4：时间推进与光流渲染

— 实现方案（对齐正文 3.2/3.3 + 补充材料 A.1）

> 本阶段重点把“时间推进 + 可微渲染 + 图像平面速度/光流渲染”链路跑通，用于：
> (1) 插值/未来帧渲染；(2) 可视化运动；(3) 为阶段5的动态重加权提供 `v_r`。

### 4.1 目标与成功标准（门禁）

- 目标：在任意目标时间 `t'`（插值/未来）下，对预测高斯进行时间推进并渲染 `RGB/Depth`；同时在图像平面渲染用于可视化与动态重加权的 **Rendered Velocity `v_r`**（补充材料 A.1）。
- 成功标准（门禁）分两类（避免相机口径混淆）：
  - 门禁 A（固定相机 pose，强门禁，推荐）：
    - 固定相机为某个参考帧 `frame_ref` 的位姿 `T_cam_world(frame_ref)`，对多个 `t'` 连续渲染 `RGB(t')` 与 `v_r(t')`。
    - 期望：静态背景 `|v_r|≈0`；动态车辆/行人 `v_r` 方向与相邻渲染帧的差分方向一致。
    - 期望：对 `v_r` 做 `clip [0,10]` 后不出现大面积饱和（幅值分布稳定）。
  - 门禁 B（逐帧相机 pose，对齐检查，弱门禁）：
    - 每个 `t'` 使用该帧相机位姿 `T_cam_world(frame_k)` 渲染。
    - 期望：`v_r` 主要高亮动态 actor 区域；**不要求**背景 `|v_r|≈0`（相机自运动会导致背景 RGB 变化）。

### 4.2 输入/输出约定（统一口径，避免后续歧义）

**输入：**

- 预测高斯（已完成阶段3的残差更新与激活前参数）：`G = [p(3), q(4), s(3), o(1), c(3)]`，以及每个高斯的捕获时间 `t_i∈[0,1]`。
- 速度：
  - Flux4D-base：网络在 `ego0` 下预测 `v_ego0∈R^3`，并在输出阶段用 `R_world_ego0` 旋转得到 `v_world`（用于推进/渲染/评测）。
  - 目标时间：`t'∈[0,1]`（插值/未来）。
- 时间步长（用于把速度转为像素位移）：`Δt_norm`，默认取相邻帧的归一化时间差：
  - `Δt_norm(k) = t_norm[k+1] - t_norm[k]`；末帧可复用上一间隔。

> 约定：`v_world` 的单位为 **米 / 归一化时间**（与阶段3的线性推进一致）。

**输出：**

- `RGB(t')`、`Depth(t')`：使用 3DGS/gsplat 渲染。
- `Alpha(t')`：累计不透明度（来自 gsplat），用于可见性/权重掩码（`valid_mask`）。
- `Rendered Velocity v_r(t')`：图像平面的 **渲染位移**（2 通道，单位：pixels），默认对应 `t'→t'+Δt_norm` 的像素位移。
- `|v_r|(t')`：位移幅值图（用于重加权与调试）。
- `valid_mask(t')`：可见性/权重掩码（默认使用 `Alpha(t')`）。

> 说明：补充材料 A.1 明确用于动态重加权的量是 `v_r`（rendered velocity in image space），并对其做 `clip [0,10]` 后参与损失加权。工程实现上若采用“像素位移”口径，`clip [0,10]` 与可视化更直观且与论文图一致。

### 4.3 时间推进（线性运动模型，正文 Eq.(2)）

对任意目标时间 `t'`，每个高斯中心从其捕获时间 `t_i` 推进到 `t'`：

- **线性模型（Flux4D-base）：** `p_world(t') = p_world(t_i) + v_world * (t' - t_i)`

实现注意事项：

- `t_i`、`t'` 均使用 clip 内 **min-max 归一化到 [0,1]** 的时间（阶段2已约定）。
- 网络输出 `v_ego0` 后需旋转为 `v_world`：`v_world = R_world_ego0 · v_ego0`（速度仅旋转，不加平移）。
- 渲染与投影统一在 `world` 下进行：使用 `p_world(t')` 与相机位姿 `T_cam_world`。

### 4.4 多时间步渲染流程（插值/未来通用）

为贴近论文评测设置，建立统一的渲染接口：

- 给定输入帧集合（例如 `{0,2,4,6,8,10}`），对目标帧：
  - 插值：`{1,3,5,7,9}`
  - 未来：`{11,12,13,14,15}`
- 对每个目标时间 `t'`：
  1) 由 `p_i(t')` 构造 `G(t')`（仅更新位置，其他属性沿用阶段3输出并做激活/稳定化）。
  2) 渲染得到 `RGB(t')`、`Depth(t')`。

门禁可视化：

- 连续渲染 `t0..tN`（或 `frame_ids` 列表），检查动态目标边缘是否随时间一致推进。
- 固定相机 pose（门禁 A）下，额外输出相邻渲染帧差分图 `|RGB(t_k)-RGB(t_{k-1})|`，用于核对 `v_r` 方向一致性。

### 4.5 Rendered Velocity / Rendered Flow 的定义与实现（补充材料 A.1 可对齐）

补充材料 A.1 使用的是 `v_r`（rendered velocity in image space）做逐像素重加权：

- `L_rgb_vw = (1 + ||sg(v_r)||) * L_rgb`，并 `clip(v_r, [0,10])` 保持稳定。

本复现对 `v_r` 给出一个**可实现且可验证**的严格定义（用于阶段4/5一致性），采用：
- **动态速度口径（推荐）**：使用同一相机 pose（固定在 `t'`）计算 `v_r`，使静态背景 `|v_r|≈0`，便于门禁与后续动态重加权。
- **单位选择**：`v_r` 表示 `t'→t'+Δt_norm` 的像素位移（pixels），并对向量范数裁剪到 `[0,10]`。

严格定义如下（解析 Jacobian，避免有限差分对 `Δt` 过敏）：

1) 对每个高斯，先用阶段3输出在 world 下推进到目标时间 `t'`：得到 `p_world(t')` 与 `v_world`。
2) 将点与速度转到相机坐标系（只旋转速度，不加平移）：
   - `p_cam = R_wc · p_world(t') + t_wc`
   - `v_cam = R_wc · v_world`
3) pinhole 投影（像素坐标）：
   - `u = fx * x/z + cx`
   - `v = fy * y/z + cy`
4) 对归一化时间求导，得到像素速度（pixels / norm_time）：
   - `du/dt = fx * (v_x * z - x * v_z) / z^2`
   - `dv/dt = fy * (v_y * z - y * v_z) / z^2`
5) 转为像素位移（pixels）：
   - `v_r,i(t') = [du/dt, dv/dt] * Δt_norm`
6) 按补充材料稳定化：对 `||v_r,i||` 做向量范数裁剪到 `[0,10]`（保持方向不变），然后用与 RGB 相同的 splat/alpha 权重渲染为图像 `v_r(t')` 与 `valid_mask(t')`。

> “Rendered Flow” 口径：在本定义下，可直接把 `v_r` 视作 `t'→t'+Δt_norm` 的像素位移（flow），无需额外乘 `Δt`。

调试门禁（必须做）：

- 静态区域：`|v_r|` 应接近 0。
- 动态车辆/行人：`v_r` 方向与相邻两帧渲染图像的位移方向一致（可用箭头/HSV 可视化）。

### 4.6 与后续阶段（阶段5动态重加权）的接口约定

- 阶段4输出的 `|v_r|` 与 `valid_mask` 将直接用于阶段5的动态重加权：
  - 仅在 `valid_mask` 有效区域计算权重；
  - 对 `|v_r|` 做平滑/裁剪（优先按补充材料的 `[0,10]`），避免极端像素主导梯度。

### 4.7 工程落点（对应模块与函数）

- 文件：`src/flux4d/render/flux4d_renderer.py`
  - `apply_linear_motion(gaussians, velocities, t_target) -> gaussians_t`
  - `render_gsplat(gaussians, camera, ...) -> (rgb, depth, alpha)`
  - `render_rendered_velocity_map(gaussians_world_t, velocities_world, camera, cfg, delta_t_norm) -> (v_r, |v_r|, valid_mask)`
    - 其中 `valid_mask` 优先使用 gsplat 的 `alpha`。
- 文件：`tools/vis/vis_flow.py`
  - 输出（门禁材料）：
    - `RGB(t')`、`Depth(t')`；
    - `|v_r|(t')` 与 `v_r` 的 HSV 可视化；
    - 固定相机 pose 下的相邻渲染帧差分图 `|RGB(t_k)-RGB(t_{k-1})|`（用于门禁 A 的“方向一致性”检查）。

### 4.8 验收清单（实现者按此自检）

- 文档中明确了：时间推进公式、`v_r` 的定义、`Δt` 的来源、裁剪范围 `[0,10]` 与其用途（动态重加权）。
- 文档中明确了：阶段4产物（RGB/Depth/Rendered Velocity/valid mask）与阶段5接口。
- 文档中明确了：至少 2 个可视化门禁（静态近零、动态方向一致）。

# 模块–文件–函数级 TODO 列表

## 顶层结构设计

```
.
├── configs/                 # 训练/评测配置
├── data/
│   ├── metadata/            # full/tiny clip 索引
│   └── cache/               # Lift 缓存（可选）
├── src/
│   └── flux4d/
│       ├── datasets/        # PandaSet/Waymo 读取与对齐
│       ├── lift/            # LiDAR -> Gaussians
│       ├── render/          # 时间推进 + 3DGS 渲染 + 光流
│       ├── models/          # 核心模型（复用 GaussianSTORM 思路）
│       ├── losses/          # 光度/速度/动态重加权
│       ├── metrics/         # PSNR/SSIM/Depth/Scene Flow 指标
│       ├── engine/          # 训练与评测循环
│       └── utils/           # 通用工具与数值辅助
├── scripts/
│   ├── preprocess_flux4d.py # 生成 full/tiny 索引
│   ├── train_flux4d.py      # 训练入口
│   └── infer_flux4d.py      # 推理与导出
├── tools/
│   └── vis/                 # 投影/深度/光流/速度可视化
├── tests/                   # 关键模块单测（后续补齐）
├── docs/                    # 文档与实验记录
├── assets/                  # 图表与可视化产物（可选）
└── third_party/             # 外部依赖仓库（如 gsplat/3DGS）
```

## 模块：配置（configs/）

- 文件：`configs/flux4d.py`
  - 说明：全阶段单文件配置，按 `data/init/coord/voxel/model/render/loss/train/eval` 分组，支持派生变量（例如由 `point_cloud_range` 与 `voxel_size` 计算 `voxel_shape`）。

## 模块：数据索引与缓存（data/）

- 文件：`data/metadata/pandaset_full_clips.pkl`
  - 字段：clip_id/timestamps/intrinsics/extrinsics/image_paths/lidar_paths
- 文件：`data/metadata/pandaset_tiny_clips.pkl`
  - 字段：与 full 一致，记录调试片段
- 目录：`data/cache/lift/`
  - 内容：G_init/尺度/投影缓存（可选）

## 模块：数据与索引（PandaSet）

- 文件：`src/flux4d/datasets/pandaset_clips.py`
  - 函数：`build_pandaset_clip_index(data_root, out_pkl_full, out_pkl_tiny, clip_len_s, stride)`
  - 函数：`load_clip(meta_entry) -> ClipData`
  - 函数：`sample_views_for_train(clip, num_views, strategy) -> ViewBatch`

## 模块：LiDAR → 高斯（Lift）

- 文件：`src/flux4d/lift/lift_lidar.py`
  - 函数：`voxel_downsample_points(points, voxel_size) -> points_ds`
  - 函数：`compute_knn_mean_distance(points, k) -> scale`
  - 函数：`project_points_to_cameras(points, intrinsics, extrinsics) -> uv_depth`
  - 函数：`sample_colors(images, uv) -> colors`
  - 函数：`create_gaussians_from_lidar(points, colors, scales, t_i) -> Gaussians`
  - 函数：`add_sky_and_far_points(gaussians, radius, num_points) -> Gaussians`
  - 函数：`build_initial_gaussians_for_clip(clip) -> G_init`

## 模块：核心模型（Flux4D-base/Flux4D）

- 文件：`src/flux4d/models/gaussian_voxelizer.py`
  - 函数：`voxelize_gaussians(gaussians, voxel_size) -> SparseTensor`
  - 函数：`build_voxel_features(gaussians) -> features`
- 文件：`src/flux4d/models/flux4d_unet.py`
  - 函数：`build_sparse_unet(cfg) -> model`
  - 函数：`forward(voxel_tensor, gaussians) -> (gaussians_refined, velocities, aux)`
- 文件：`src/flux4d/models/iterative_refine.py`
  - 函数：`compute_gaussian_gradients(render_outputs, targets) -> grad_tensor`
  - 函数：`refine_step(gaussians, gradients) -> gaussians_refined`
  - 函数：`iterative_refine(gaussians, gradients, num_iters) -> gaussians_final`
- 文件：`src/flux4d/models/flux4d_model.py`
  - 函数：`forward(G_init, T, refine_iters) -> (G, V, aux)`

## 模块：时间推进与渲染

- 文件：`src/flux4d/render/flux4d_renderer.py`
  - 函数：`apply_linear_motion(gaussians, velocities, t_target) -> gaussians_t`
  - 函数：`apply_polynomial_motion(gaussians, motion_params, t_target) -> gaussians_t`
  - 函数：`render_rgb_depth(gaussians, camera) -> (rgb, depth)`
  - 函数：`render_flow_map(gaussians, velocities, camera, t_src, t_tgt) -> flow`

## 模块：损失与动态重加权

- 文件：`src/flux4d/losses/flux4d_losses.py`
  - 函数：`photometric_l1_loss(pred, target, weight_map=None) -> loss`
  - 函数：`ssim_loss(pred, target, weight_map=None) -> loss`
  - 函数：`depth_l1_loss(depth_pred, lidar_proj_depth) -> loss`
  - 函数：`velocity_regularization(velocities) -> loss`
  - 函数：`build_dynamic_weight_map_from_flow(flow, alpha, clip) -> weight_map`
  - 函数：`total_loss(...) -> loss_dict`

## 模块：指标（Metrics）

- 文件：`src/flux4d/metrics/metrics.py`
  - 函数：`compute_psnr(rgb_pred, rgb_gt) -> float`
  - 函数：`compute_ssim(rgb_pred, rgb_gt) -> float`
  - 函数：`compute_depth_rmse(depth_pred, depth_gt) -> float`
  - 函数：`compute_epe3d(flow_pred, flow_gt) -> float`
  - 函数：`compute_acc5(flow_pred, flow_gt) -> float`
  - 函数：`compute_acc10(flow_pred, flow_gt) -> float`
  - 函数：`compute_angular_error(flow_pred, flow_gt) -> float`
  - 函数：`compute_epe_3way(flow_pred, flow_gt, masks) -> Dict[str, float]`
  - 函数：`compute_bucketed_normalized_epe(flow_pred, flow_gt, buckets) -> Dict[str, float]`

## 模块：训练与评测引擎

- 文件：`src/flux4d/engine/trainer.py`
  - 函数：`train_loop(cfg) -> None`
  - 函数：`eval_loop(cfg) -> metrics`
  - 函数：`save_debug_renders(batch, outputs, out_dir)`
- 文件：`src/flux4d/engine/checkpoint.py`
  - 函数：`save_ckpt(state, out_path) -> None`
  - 函数：`load_ckpt(path, map_location) -> state`

## 模块：通用工具

- 文件：`src/flux4d/utils/config.py`
  - 函数：`load_config(path) -> Cfg`
  - 函数：`merge_overrides(cfg, overrides) -> Cfg`
- 文件：`src/flux4d/utils/geometry.py`
  - 函数：`compose_extrinsics(R, t) -> T`
  - 函数：`transform_points(points, T) -> points_t`

## 模块：脚本入口

- 文件：`scripts/preprocess_flux4d.py`
  - 函数：`main() -> None`
- 文件：`scripts/train_flux4d.py`
  - 函数：`main() -> None`
- 文件：`scripts/infer_flux4d.py`
  - 函数：`main() -> None`

## 模块：可视化工具

- 文件：`tools/vis/vis_lift.py`
  - 函数：`main() -> None`
- 文件：`tools/vis/vis_flow.py`
  - 函数：`main() -> None`
- 文件：`tools/vis/make_video.py`
  - 函数：`main() -> None`

## 模块：测试

- 文件：`tests/test_lift.py`
  - 函数：`test_voxel_downsample() -> None`
  - 函数：`test_knn_mean_distance() -> None`
- 文件：`tests/test_render.py`
  - 函数：`test_render_shapes() -> None`

## 模块：资源与第三方

- 目录：`assets/`
  - 内容：可视化图表与视频产物
- 目录：`third_party/`
  - 内容：gsplat/3DGS 等外部依赖（按需引入）

# 验证与可视化策略

## 阶段验证门禁

- 阶段0：GaussianSTORM demo 渲染正确；PandaSet 数据可读。
- 阶段2：G_init 渲染与原图结构一致；深度与投影 LiDAR 对齐。
- 阶段3：tiny clip 过拟合成功（PSNR/SSIM 上升，速度分布稳定；回归产物见 `assets/vis/stage3_overfit_run2/`）。
- 阶段4：插值帧稳定；Flow 可视化方向与相邻帧位移一致。
- 阶段5：迭代 refinement 提升纹理细节；动态区域误差下降。

## 4D 动态场景可视化

- 渲染 t0..tN 的 RGB/Depth/Flow 序列并合成视频。
- 生成新视角轨迹视频，检查时空一致性与遮挡处理。
- 对动态主体做局部放大对比：迭代前后、动态权重前后。

## tiny 数据集快速验证

- 选 1–2 个短片段，固定视图数训练到过拟合。
- 监控重建 loss、速度范数、渲染差分图。
- 若无法过拟合，优先排查时间同步、坐标系与渲染链路。

# 风险与优先级

## 风险清单（按优先级）

- P0 数据对齐错误（时间戳/外参错位）
  - 对策：帧级可视化对齐检查，叠加点云到图像验证。
- P0 显存不足（24G 调试批量过大）
  - 对策：减小 batch、梯度累积、混合精度、激活检查点。
- P0 渲染/损失实现偏差
  - 对策：先复现 Flux4D-base，固定权重逐项对齐损失。
- P1 LiDAR 稀疏导致外观欠拟合
  - 对策：多帧聚合、随机点/单目深度辅助。
- P1 动态重加权不稳定
  - 对策：权重裁剪与平滑；先在 tiny 上验证稳定性。
- P1 速度预测退化或发散
  - 对策：监控 v_i 分布，调节 λ_vel 与学习率。
- P2 迭代优化引入伪影
  - 对策：限制迭代次数与梯度幅度，保留 baseline 对比。
- P2 运动模型过简（线性假设不足）
  - 对策：启用多项式运动并做消融验证。
- P2 评测指标难复现
  - 对策：严格按论文评测协议采样，并保留可视化趋势证据。
- P3 工程复杂度过高
  - 对策：优先最小可跑通版本，逐步叠加增强模块。
