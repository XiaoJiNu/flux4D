# flux4D复现计划

# 背景

论文路径：docs/论文中202512_Flux4D.pdf是原始论文，Flux4D-基于光流的无监督4D重建-翻译.md是翻译的文档。

目标：复现flux4d论文的代码，达到论文的指标。

数据路径：

1. pandaset数据：/home/yr/yr/data/automonous/pandaset
2. waymo数据：暂时未下载

先以pandaset数据来复现，a. 先生成完整的数据标签，b. 同时要生成一个小的数据标签，比如只提取一个场景或者两个场景数据的标签，在代码调试期间，使用这个小数据来调试代码

# 初版计划-MiroThinker

下面直接回答你的需求：**有没有现成“可改一改就变 Flux4D”的好仓库？以及，给你一个可执行的复现方案（不写代码，只讲步骤与设计）。**

---

## 一、代码基础现状：有没有“官方 Flux4D 仓库”？

根据目前公开信息：

* Flux4D 论文（NeurIPS 2025）已经上线，但 **作者暂未公开官方代码仓库** （个人主页、Arxiv、项目页都只有论文 / teaser，暂无 code 链接）。
* 社区里有一个名字相似的 **Flow4D** 仓库（`dgist-cvlab/Flow4D`），但那是  **LiDAR 场景流（Flow4D: Leveraging 4D Voxel Network for LiDAR Scene Flow Estimation, RA-L 2025）** ，不是 Flux4D 这篇 3DGS + 4D 重建论文。
* 有很多 **3D Gaussian Splatting 官方实现** 和  **稀疏卷积 / 场景流 / 4D 方法代码** ，可以作为构建 Flux4D 的“乐高积木”。

结论：

> 目前没有“直接就是 Flux4D 的官方开源代码”，你需要 **基于已有成熟项目自己组装一个 Flux4D 风格的系统** 。好在 3DGS + 稀疏卷积 + 自动驾驶数据这几块都有非常成熟的仓库可用。

---

## 二、推荐的“起步基础仓库”组合

你的目标是：**找一个/几个现成的好仓库，在此基础上做少量结构改造，就能逼近 Flux4D。**

可以按模块拆分来看：

### 1. 渲染与 3D 高斯核心：3D Gaussian Splatting 官方实现

* 仓库：`graphdeco-inria/gaussian-splatting`
* 提供：
  * 完整的 3D 高斯表示（位置、尺度、方向、颜色、opacity）
  * CUDA 加速渲染（前向 + 可微梯度）
  * 数据结构和优化 loop（虽然是 per-scene 优化，但 renderer 本身完全可用）

你可以：

* **直接复用其高斯数据结构 & CUDA 渲染器** ；
* 不再用它的 per-scene 优化流程，而是 **用你自己的 Flux4D 网络来产出高斯参数和速度** ，然后调用它的 rasterizer 做前向 + 反向。

### 2. 稀疏卷积 + 3D U-Net：MinkowskiEngine 或 spconv

* MinkowskiEngine / spconv 都是成熟的  **稀疏 3D 卷积库** ，支持 3D U-Net。
* Flux4D 需要的就是类似：
  * 点云 / 高斯 → 体素 → 稀疏 3D U-Net → 回到点 / 高斯上输出参数和速度。

你可以选择：

* **MinkowskiEngine** ：接口成熟，社区丰富；
* 或  **spconv** ：很多自动驾驶 3D 检测 / 分割项目使用，生态成熟。

### 3. 自动驾驶多传感器数据：PandaSet & Waymo 工具

* PandaSet：有官方 devkit（`scaleapi/pandaset-devkit`）和很多第三方工具；
* Waymo：官方 `waymo-open-dataset` 仓库，有 Python / TF 读取工具。

你需要：

* 用这些工具完成  **相机 + LiDAR 对齐（外参、时间）** ；
* 拿到 **每一帧的：多相机图像、对应 LiDAR 点云、时间戳** 。

---

## 三、整体复现思路（高层方案）

可以把复现任务拆成 5 个大模块，每个模块和你要修改/搭建的地方，我都标清楚：

1. **数据准备** ：将 PandaSet / Waymo 转成 Flux4D 所需的 “多视角图像 + LiDAR + 标准坐标系”
2. **Lift：LiDAR 点 → 初始 3D 高斯** （基于 3DGS 数据结构）
3. **Predict：3D U-Net + 稀疏卷积网络输出高斯属性 + 速度**
4. **Render：调用 3DGS 渲染器 + 加入时间运动模型**
5. **Train：实现 Flux4D 的损失（光度 + 深度 + 速度正则 + 动态重加权），做跨场景训练**

下面详细拆一下每一步怎么做、做在哪个仓库上改。

---

## 四、方案细化：逐步怎么做

### 步骤 1：数据准备与规范化

目标：把真实驾驶数据整理成一个 **统一数据格式** ，方便后续 pipeline。

1. **选择数据集** （和论文一致，优先 PandaSet + Waymo）：

* PandaSet：多相机 1080p + 64 线 LiDAR；
* Waymo：前向 3 相机 + 顶部 LiDAR。

1. **数据对齐：**
   * 使用官方 devkit，将每一个时间帧整理出：
     * 多个相机图像 `I_k^c`（c 为相机编号）
     * 对应 LiDAR 点云 `P_k`
     * 相机内参 K、外参 T_cam←world
     * LiDAR 外参 T_lidar←world
   * 确保同一时间帧下， **空间坐标统一到一个 world 坐标系** 。
2. **序列划分：**
   * 按论文做法，取连续 1.5s / 8s 的短序列 / 长序列：
     * 每条序列包含若干时间步 t_0, t_1, ..., t_T
     * 每个时间步有（多相机图像 + 对应 LiDAR）
3. **保存成统一格式** （建议用 JSON / HDF5）：

* 每条序列一个配置：
  * `frames`: 列表，包含：
    * `time`: 时间戳
    * `images`: 各相机图像路径 + 相机参数
    * `lidar_points`: 点云二进制文件路径
    * `T_cam`, `T_lidar`：外参与时间

 **你要改/写的部分** ：一个“PandaSet/Waymo → 统一格式”的预处理脚本，但完全可以照着官方 devkit 来写逻辑。

---

### 步骤 2：Lift – LiDAR 点云 → 初始 3D 高斯

这一步是“把点云变成高斯”的初始化阶段，建议 **在 3DGS 仓库之外写一个独立模块** ，但复用它的参数结构。

1. **一 LiDAR 点 → 一高斯**

   对每个 LiDAR 帧 `P_k`，对每个点 p：

   * 高斯中心：
     pi=p_worldp_i = p\_{\text{world}}**p**i=**p**_**world**
   * 尺度 scale：
     * 用 kNN（如 k=10）计算邻近点平均距离 d̄ 作为尺度基准；
   * 颜色 color：
     * 将 p 投影到所有可见相机的图像中；
     * 取一个主相机的像素 RGB 作为颜色；
   * 不透明度 opacity：初始可设 0.9 或基于 LiDAR 强度设定；
   * 时间戳 t_i：设为 LiDAR 帧采集时间；
   * 初始速度 v_i：设为 0。
2. **输出数据结构**

   对接 3DGS 的高斯结构：

   * 位置、尺度、旋转（可初始为单位）、颜色、opacity；
   * 额外存 v_i 和 t_i。
3. **存储方式**

   * 按序列保存一个 `initial_gaussians` 文件（比如 `.npz` 或 `.pt`）：
     * 数组大小约 = 所有帧 LiDAR 点数总和（+ 论文里说的少量随机远景点）。

 **你要做的修改** ：

* 在 3DGS 项目里，仿照它读写高斯参数的格式，增加你自己的“初始化 pipeline”；
* 不改 3DGS 的核心渲染逻辑，只是 **多了一个基于 LiDAR 的初始化入口** 。

---

### 步骤 3：Predict – 用 3D U-Net + 稀疏卷积预测高斯更新与速度

这是 Flux4D 的“神经网络主体”，你需要独立实现，但可以借助 MinkowskiEngine / spconv。

#### 3.1 输入与输出设计

* 输入：
  * 初始高斯集合 {gi}i=1N\{g_i\}_{i=1}^N**{**g**i****}**i**=**1**N**，包含位置、尺度、颜色等；
  * 每个高斯的时间戳 tit_i**t**i。
* 输出：
  * 更新后的高斯参数（Δ位置、Δ尺度、Δ颜色等）；
  * 每个高斯的 3D 速度向量 viv_i**v**i。

 **注意** ：高斯数量 N 随点云变化，但网络参数固定，要用**体素 + 稀疏卷积**来解决。

#### 3.2 实现步骤（抽象级别）

1. **体素化（Voxelization）** ：

* 定义固定 3D 区域（如 x,y∈[-50,50], z∈[-5,5]）；
* 选定体素分辨率（如 256×256×64）；
* 将每个高斯中心映射到体素坐标 (ix,iy,iz)；
* 在同一体素内，对高斯特征（位置偏移、颜色、scale 等）做池化，得到体素级特征。

1. **稀疏 3D U-Net** ：

* 用 MinkowskiEngine / spconv 的 SparseConv 实现一个标准的 3D U-Net：
  * 编码：多层 downsample；
  * 解码：多层 upsample + skip connection；
* 输出：每个非空体素一个高维特征（比如 64/128 维）。

1. **回到点/高斯级** ：

* 对每个高斯 i，查回它所在体素 v 的特征；
* 拼接高斯原始属性（位置、时间编码等）；
* 通过一个小 MLP 输出：
  * Δ参数（更新位置/尺度/颜色/opacity）
  * 速度向量 viv_i**v**i。

这部分的逻辑你已经在之前 Q&A 里理清楚了： **体素的数量不等于高斯数量** ，体素只是中间特征聚合层，最后输出还是“一点一输出”。

 **你要搭的东西** ：

* 一个基于 MinkowskiEngine/spconv 的 `Flux4DBackbone`（3D U-Net）；
* 一个 `PredictHead`，对每个高斯输出参数更新与速度。

---

### 步骤 4：Render – 加入时间维的 3DGS 渲染

基于 3DGS 的 renderer 做一个“小改动”：

1. **时间推进（motion model）** ：

* 对于某个目标时间 t'，每个高斯新的中心：
  pi′(t′)=pi+vi⋅(t′−ti)p_i'(t') = p_i + v_i \cdot (t' - t_i)**p**i**′(**t**′**)**=**p**i****+**v**i****⋅**(**t**′**−**t**i****)**
* 如果要支持多项式运动，可以拓展为：
  pi′(t′)=pi+vi(1)(t′−ti)+vi(2)(t′−ti)2+…p_i'(t') = p_i + v_i^{(1)}(t'-t_i) + v_i^{(2)}(t'-t_i)^2 + \dots**p**i**′(**t**′**)**=**p**i****+**v**i**(**1**)****(**t**′**−**t**i****)**+**v**i**(**2**)****(**t**′**−**t**i****)**2**+**…**
* 初始实现建议只上 **线性模型** ，已经足够复现论文主结果。

1. **渲染 RGB + 深度** ：

* 把 `p_i'(t')` 丢进 3DGS 的 renderer，正常渲染：
  * 输出彩色图像 I^(t′)\hat{I}(t')**I**^**(**t**′**)；
  * 输出深度图 D^(t′)\hat{D}(t')**D**^**(**t**′**)。

1. **渲染光流（flow）** （用于动态重加权）：

* 对前后两个时间 (t) 和 (t')，分别渲染投影坐标：
  * `proj(p_i(t))` 和 `proj(p_i(t'))`；
* 差值就是 2D 光流向量；
* 汇聚到像素栅格上得到光流图 F^(x,y)\hat{F}(x,y)**F**^**(**x**,**y**)**。

 **你要做的改动点** ：

* 为 3DGS renderer 增加一个前置步骤：对位置 `p_i` 按速度推进到目标时间；
* 增加一个“光流渲染模式”，只存像素运动向量，而不渲染颜色。

---

### 步骤 5：Train – 损失设计与训练流程

这一步是复现 Flux4D 最核心的“思想”：

#### 5.1 光度损失（photometric loss）

* 对 RGB：
  * L1：∥I^−I∥1\|\hat{I} - I\|_1**∥**I**^**−**I**∥**1**
  * SSIM：1−SSIM(I^,I)1-SSIM(\hat{I}, I)**1**−**SS**I**M**(**I**^**,**I**)**
* 对深度：
  * LiDAR 投影到图像平面得到深度 GT 图 (D)，与 D^\hat{D}**D**^ 做 L1。

整体：

Lrecon=λL1L1+λSSIM(1−SSIM)+λdepthLdepthL_{\text{recon}} = \lambda_{L1} L_1 + \lambda_{SSIM}(1-SSIM) + \lambda_{depth} L_{depth}**L**recon=**λ**L**1****L**1****+**λ**SS**I**M****(**1**−**SS**I**M**)**+**λ**d**e**pt**h****L**d**e**pt**h****

#### 5.2 速度正则 LvelL_**L**v**e**l

* 核心公式：
  Lvel=1M∑i∥vi∥2L_{vel} = \frac{1}{M} \sum_i \|v_i\|^2**L**v**e**l=**M**1****i**∑****∥**v**i****∥**2
* 作用：鼓励“能不动就别动”，只让真正需要运动的点承担速度。

#### 5.3 动态区域重加权

* 用渲染出来的光流模长 ∥F^(x,y)∥\|\hat{F}(x,y)\|**∥**F**^**(**x**,**y**)**∥** 作为权重：
  w(x,y)∝∥F^(x,y)∥w(x,y) \propto \|\hat{F}(x,y)\|**w**(**x**,**y**)**∝**∥**F**^**(**x**,**y**)**∥
* 加权后的光度损失：
  Lreconweighted=∑x,yw(x,y) ℓ(I^(x,y),I(x,y))L_{\text{recon}}^{\text{weighted}} = \sum_{x,y} w(x,y)\,\ell(\hat{I}(x,y), I(x,y))**L**recon**weighted****=**x**,**y**∑w**(**x**,**y**)**ℓ**(**I**^**(**x**,**y**)**,**I**(**x**,**y**))
  其中 ℓ\ell**ℓ** 是 L1+SSIM 的组合。

#### 5.4 总损失

L=Lreconweighted+λvelLvelL = L_}^} + \lambda_ L_**L**=**L**recon**weighted****+**λ**v**e**l****L**v**e**l****

## 五、实际执行路径建议（给你一条“从零到有”的路线）

如果你现在开始做，比较现实的路线是：

### 阶段 0：快速原型（1–2 周）

* 在 3DGS 官方实现上：
  * 先用 **静态场景 + 简单 LiDAR/深度初始化** 跑通：
    * “用点云初始化高斯 → 3DGS 渲染 → photometric loss 训练微调高斯”；
  * 这一步只是确认你能正确调用 renderer 与反向传播。

### 阶段 1：实现 Lift + 静态版本（2–3 周）

* 对一小段 PandaSet 序列：
  * 写出完整的 Lift 流程（LiDAR → 高斯）；
  * 不引入速度，只做 per-frame 高斯优化 + 光度损失；
  * 确认：
    * 每帧重建质量 OK；
    * 时间维上虽然未建模运动，但至少几何是合理的。

### 阶段 2：加入 Predict 网络（3–4 周）

* 实现基于 MinkowskiEngine / spconv 的 3D U-Net；
* 用“单帧”或“短序列”做训练：
  * 输入：多帧 LiDAR 初始化的高斯（带时间戳）；
  * 输出：高斯更新（可先不预测速度）；
  * 损失：先只用静态光度损失，让网络学会 refine gaussians。

### 阶段 3：引入速度与 4D 损失（4–6 周）

* 在网络输出中加入 3D 速度向量；
* 修改渲染：加入时间推进 p_i(t')；
* 接入：
  * 速度正则 LvelL_{vel}**L**v**e**l；
  * 光流渲染 + 动态区域重加权；
* 先在 **1.5s 的短序列** 上训练，观察：
  * 静态背景是否速度接近 0；
  * 车辆/行人是否有明显非零速度。

### 阶段 4：扩展到长序列 + 泛化训练（>6 周）

* 在多条 PandaSet / Waymo 序列上做跨场景训练：
  * 随机采样场景 -> 随机采样一段时间窗口；
  * 前馈一次网络完成 reconstruction 与 flow；
* 评估：
  * PSNR/SSIM / Depth RMSE / EPE3D 等指标；
  * 与论文中的表格进行对比（趋势和量级要接近）。

---

## 六、回答你的原始问题（简要总结）

> **Q：我想复现 Flux4D 论文，有没有现有的好的开源代码，让我在那个基础上进行修改，然后复现 Flux4D 论文？**

**A：**

1. **目前还没有 Flux4D 的官方开源仓库** ，也没有一个“一键变 Flux4D”的现成项目。
2. 但你可以**非常实际地**基于如下组合构建自己的 Flux4D 实现：
   * 3D 高斯渲染与数据结构：**`graphdeco-inria/gaussian-splatting`**
   * 稀疏 3D U-Net（处理变长高斯集合）：**MinkowskiEngine / spconv**
   * 自动驾驶多传感器数据处理：**PandaSet / Waymo 的官方 devkit + 第三方工具**
3. 在这个基础上，你需要做的主要工作是：
   * 写一个  **LiDAR → 高斯的 Lift 模块** ；
   * 实现一个  **稀疏 3D U-Net + 点级 MLP 的 Predict 网络** ，输出高斯参数和速度；
   * 在 3DGS 渲染器上加入  **时间推进 + 光流渲染** ；
   * 实现 Flux4D 的  **光度损失 + 速度正则 + 动态重加权** ，并搭好跨场景训练流程。
4. 从工程量上看，这是一个**“中到大型”复现项目**，但 **不需要从零写渲染器和稀疏卷积** ，更多是“拼装 + 接口设计 + 损失工程”，完全在研究生/工程师可控范围内。

如果你愿意，我可以在下一步帮你细化成一个更具体的“开发任务清单”（按模块拆分到函数级的 TODO），但不写具体代码。

下面给的是一个**以 GaussianSTORM 仓库为主参考、面向“Flux4D 类方法”复现/改造**的开发任务清单。

特点：

* 充分对齐 GaussianSTORM 的工程结构（`main_storm.py / engine_storm.py / preprocess.py / inference.py / storm/` 等；本项目对应 `models/`）
* 又显式加入你想实现的 Flux4D 思路（LiDAR→高斯 Lift、速度正则、动态区域重加权、基于 3D 高斯+速度的 4D 重建）
* 粒度到 **模块–文件–函数级 TODO** ，不写具体代码

你可以直接把这些 TODO 拆成 issue 或任务卡来实施。

---

# 顶层结构建议-MiroThinker

和 GaussianSTORM 保持类似的顶层结构，再加上你需要的新模块：

* `models/`：核心模型（时空编码器 + 高斯/速度预测，对应 GaussianSTORM 的 `storm/`）
* `engine_flux4d.py`：训练/评估引擎（可参考 `engine_storm.py`）
* `main_flux4d.py`：训练入口脚本（参考 `main_storm.py`）
* `inference_flux4d.py`：推理/可视化入口（参考 `inference.py`）
* `preprocess_flux4d.py` / `preproc/`：数据预处理（可复用/扩展 `preprocess.py`）
* `lift/`：LiDAR 点云 → 初始 3D 高斯
* `render/`：基于 3DGS/gsplat 的时序渲染与光流渲染
* `losses/`：光度损失 + 速度正则 + 动态重加权
* `data/`：数据集列表、clip 索引
* `docs/`：使用文档（可在已有 `docs/` 中添加 Flux4D 说明）

下面按模块拆分到函数级 TODO。

---

## 模块一：数据与预处理（基于 GaussianSTORM 的 `preprocess.py` / `preproc/` 扩展）

### 1.1 Waymo/PandaSet 序列加载与对齐

 **文件** ：`preprocess_flux4d.py` 或在原 `preprocess.py` 中新增函数

* [ ] `load_sequence_metadata(dataset_root, sequence_id) -> SequenceMeta`
  * 读取一段序列的：
    * 相机图像路径/时间戳
    * LiDAR 点云路径/时间戳
    * 相机内参/外参、LiDAR 外参
  * 输出抽象结构 `SequenceMeta`
* [ ] `sync_cams_lidar(sequence_meta, max_time_diff) -> SyncedFrames`
  * 对每个时间点，将多相机帧和最近的 LiDAR 帧对齐
  * 返回按时间排序的 `SyncedFrame` 列表（每个含：多相机 + LiDAR）
* [ ] `split_into_clips(synced_frames, clip_len_seconds, stride_seconds) -> List[ClipMeta]`
  * 将长序列按时间长度切分成多个 clip（例如 1.5s / 8s）
  * 每个 `ClipMeta` 保存：
    * 帧索引范围
    * 起始/结束时间
    * 所属序列 ID
* [ ] `export_clip_index(clip_meta_list, out_path)`
  * 将所有 clip 的信息保存成 JSON/YAML，训练时直接读取

### 1.2 天空/地面等额外预处理（可复用 `extract_sky.py`）

 **文件** ：`extract_sky.py`

* [ ] `estimate_sky_mask(image, method='segment_anything' or 'depth') -> np.ndarray[H,W]`
  * 基于已有 sky 提取脚本：
    * 输入单张 RGB 图像
    * 输出 0/1 的天空 mask
* [ ] `generate_sky_region_stats(sequence_meta) -> SkyRegionInfo`
  * 统计序列中天空区域的大致空间范围（例如远处的大平面）

---

## 模块二：LiDAR → 初始 3D 高斯 Lift（新建 `lift/` 模块）

### 2.1 单帧点云预处理

 **文件** ：`lift/lift_lidar.py`

* [ ] `voxel_downsample_points(points, voxel_size) -> np.ndarray`
  * 对 LiDAR 点做体素下采样（控制初始高斯数量）
* [ ] `compute_knn_mean_distance(points, k) -> np.ndarray`
  * 对每个点计算 k 近邻的平均距离
  * 为每个点得到一标量或三维尺度基准

### 2.2 点云 + 图像 → 高斯属性

* [ ] `project_point_to_cameras(point_world, cam_params_list) -> Optional[PixelSample]`
  * 将世界坐标点投影到多相机
  * 找到最佳相机及像素坐标、深度
* [ ] `sample_color_from_images(pixel_sample, image_dict) -> np.ndarray[3]`
  * 根据投影像素从图像中读取 RGB
  * 可做边界和插值处理
* [ ] `create_gaussian_from_point(point_world, t_frame, mean_knn_dist, color) -> GaussianParam`
  * 组装单个高斯参数：
    * `pos = point_world`
    * `scale`：基于 `mean_knn_dist`（如 x,y,z 各方向相同）
    * `rot`：单位四元数
    * `color = RGB`
    * `opacity`：初始设为常数/依赖 LiDAR 强度
    * `t = t_frame`
    * `vel = 0`（初始化速度）

### 2.3 整帧/整 clip 的 Lift

* [ ] `lift_lidar_frame_to_gaussians(lidar_points, t_frame, cam_params_list, images_dict) -> List[GaussianParam]`
  * 对单帧 LiDAR 调用：
    * `voxel_downsample_points`
    * `compute_knn_mean_distance`
    * `project_point_to_cameras`
    * `sample_color_from_images`
    * `create_gaussian_from_point`
  * 返回该帧所有高斯列表
* [ ] `add_far_sky_gaussians(bbox, sky_region_info, num_sky_gaussians) -> List[GaussianParam]`
  * 使用 `SkyRegionInfo` 在远处生成少量高斯
  * 模型用于天空/远景表示
* [ ] `build_initial_gaussians_for_clip(clip_meta, dataset_root) -> List[GaussianParam]`
  * 对 clip 中每个时间帧：
    * 读取 LiDAR + 图像
    * 调 `lift_lidar_frame_to_gaussians` 获得高斯
  * 合并所有帧的高斯
  * 可附加 `add_far_sky_gaussians`
  * 返回 `G_init`
* [ ] `save_initial_gaussians(G_init, out_path)`
* [ ] `load_initial_gaussians(path) -> List[GaussianParam]`

---

## 模块三：核心模型（在 `models/` 内扩展 Flux4D-style 模型）

假定在 `models/` 目录下新建/扩展为 `flux4d_model.py`，整体风格和 GaussianSTORM 中 STORM 模型一致（Transformer / Perceiver + Motion Tokens），但我们要输出 **高斯参数 + 速度** 。

### 3.1 高斯–体素–稀疏特征编码

 **文件** ：`models/flux4d_encoder.py`

* [ ] `build_voxel_grid_config(world_bbox, grid_resolution) -> VoxelGridConfig`
  * 设定整体空间范围（参考数据集坐标系）
  * 决定 3D 网格分辨率 (e.g., 256×256×64)
  * 计算各方向 `voxel_size`
* [ ] `gaussians_to_voxel_indices(gaussians, voxel_cfg) -> np.ndarray[N,3]`
  * 把每个高斯中心 `pos` 映射成体素整数坐标
* [ ] `aggregate_gaussian_features_to_voxels(gaussians, voxel_cfg) -> (voxel_coords, voxel_feats, point2voxel_idx)`
  * 定义点级初始特征（如 color、scale、opacity、time encoding 等）
  * 在同一体素中对多个高斯特征做 pooling（mean/max）
  * 输出：
    * `voxel_coords`：非空体素坐标
    * `voxel_feats`：对应特征向量
    * `point2voxel_idx`：每个高斯对应体素 ID
* [ ] `build_sparse_tensor_from_voxels(voxel_coords, voxel_feats) -> SparseTensor`
  * 使用 MinkowskiEngine / spconv 构造稀疏 3D 张量

### 3.2 时空 Transformer / Perceiver 编码

 **文件** ：`models/flux4d_transformer.py`

* [ ] `encode_time_scalar(t_i) -> time_feat`
  * 将标量时间戳做 positional encoding
  * 输出固定维度时间特征
* [ ] `build_motion_tokens(num_tokens, dim) -> MotionTokens`
  * 初始化一组可学习的 motion tokens
  * 对应 STORM 里的 motion token 概念
* [ ] `build_sky_token(dim) -> SkyToken`
  * 单一可学习天空 token（可参考 GaussianSTORM 中的 `--use_sky_token` 思路）
* [ ] `build_affine_token(dim) -> AffineToken`
  * 全局仿射变换 token，对场景整体运动和相机轨迹建模
* [ ] `Flux4DTransformer.__init__(...)`
  * 定义 Transformer / Perceiver 结构：
    * 多层 self-attention / cross-attention
    * 输入：体素特征 + 各类 token（motion / sky / affine / time）
* [ ] `Flux4DTransformer.forward(voxel_sparse_tensor, motion_tokens, sky_token, affine_token, time_tokens) -> voxel_features_out, global_tokens_out`
  * 对体素特征和 tokens 进行联合编码
  * 返回更新后的体素特征、更新后的 tokens（尤其 motion tokens）

### 3.3 点级 Head：高斯参数更新 + 速度预测

 **文件** ：`models/flux4d_head.py`

* [ ] `gather_voxel_features_for_gaussians(voxel_features_out, point2voxel_idx) -> point_feats`
  * 根据 `point2voxel_idx`，把体素特征映射回高斯级特征
* [ ] `build_gaussian_point_feature(gaussian, point_feat, time_feat, motion_token_feat) -> feature_vec`
  * 将：
    * 体素特征
    * 高斯原始属性（如 scale、颜色）
    * 时间特征
    * 与之关联的 motion token 特征（可以简单拼接）
  * 拼接为统一点级 feature
* [ ] `GaussianHead.__init__(in_dim, hidden_dim, out_dim_param, out_dim_vel)`
  * 定义点级 MLP：
    * 输出 Δpos、Δscale、Δcolor、Δopacity 等
    * 输出 3D 速度向量 vel
* [ ] `GaussianHead.forward(point_features) -> (delta_params, velocities)`
  * 批量处理所有高斯 feature
  * 输出：
    * `delta_params`：各维度的参数更新
    * `velocities`：每个高斯的 3D 速度
* [ ] `apply_gaussian_updates(G_init, delta_params) -> G_refined`
  * 把 Δ 应用到初始高斯集合，得到 refined 高斯集合

### 3.4 顶层 Flux4D 模型封装

 **文件** ：`models/flux4d_model.py`

* [ ] `Flux4DModel.__init__(voxel_cfg, transformer_cfg, head_cfg, world_bbox)`
  * 初始化：
    * 编码器（体素模块）
    * Transformer/Perceiver
    * Head
* [ ] `Flux4DModel.forward(G_init) -> (G_refined, velocities, aux_outputs)`
  * 主前向流程：
    * `gaussians_to_voxel_indices`
    * `aggregate_gaussian_features_to_voxels`
    * `build_sparse_tensor_from_voxels`
    * `Flux4DTransformer.forward`
    * `gather_voxel_features_for_gaussians`
    * `GaussianHead.forward`
    * `apply_gaussian_updates`
  * 返回：
    * `G_refined`（更新后的高斯）
    * `velocities`（每高斯 3D 速度）
    * `aux_outputs`（如 motion/sky token 特征，用于可视化/分析）

---

## 模块四：渲染与光流（参考 GaussianSTORM 所用的 gsplat）

### 4.1 基于速度的时间传播 + 3DGS 渲染

 **文件** ：`render/flux4d_renderer.py`

* [ ] `apply_linear_motion(G_refined, velocities, target_time) -> G_at_t`
  * 对每个高斯：
    * `pos_i(t') = pos_i + v_i * (t' - t_i)`
  * 生成时间 t' 下的高斯集合
* [ ] `build_gsplat_scene_from_gaussians(G_at_t, camera_params) -> GsplatScene`
  * 把高斯参数喂给 gsplat 的场景对象
  * 配置相机
* [ ] `render_rgb_depth(scene, image_size) -> (rgb, depth)`
  * 调用 gsplat 渲染：
    * 输出 RGB 图
    * 输出深度图（如支持）

### 4.2 光流渲染（用于动态区域重加权）

* [ ] `render_two_times_and_get_projections(G_refined, velocities, t0, t1, camera_params) -> (proj_t0, proj_t1)`
  * 在 t0、t1 分别：
    * `apply_linear_motion`
    * 构建 gsplat 场景，获取高斯屏幕坐标投影（可在 renderer 内部修改，保存中间结果）
* [ ] `compute_optical_flow_from_projections(proj_t0, proj_t1, image_size) -> flow_map`
  * 根据两个时间点的投影位置差，计算每个像素的 (u,v) 光流
  * 使用插值/最近邻把高斯的 2D 位移融合到像素栅格
* [ ] `render_flow_map(G_refined, velocities, t0, t1, camera_params, image_size) -> flow_map`
  * 封装上述两个步骤
  * 输出训练所需的光流图

---

## 模块五：损失与动态重加权（新建 `losses/`）

### 5.1 基础损失

 **文件** ：`losses/flux4d_losses.py`

* [ ] `photometric_l1_loss(pred_rgb, gt_rgb, mask=None) -> Tensor`
* [ ] `photometric_ssim_loss(pred_rgb, gt_rgb, window_size=11) -> Tensor`
* [ ] `depth_l1_loss(pred_depth, gt_depth, mask=None) -> Tensor`

### 5.2 速度正则

* [ ] `velocity_regularization(velocities, mask=None) -> Tensor`
  * 实现：
    Lvel=1M∑i∥vi∥2L_{vel} = \frac{1}{M} \sum_i \|v_i\|^2**L**v**e**l=**M**1****i**∑****∥**v**i****∥**2

### 5.3 动态区域重加权

* [ ] `flow_magnitude(flow_map) -> Tensor[H,W]`
  * `sqrt(u^2 + v^2)`，得到每个像素流大小
* [ ] `build_dynamic_weight_map(flow_mag, alpha, beta, normalize=True) -> Tensor[H,W]`
  * 把流大小映射为权重：
    * 例如线性/非线性缩放
    * 可选归一化到 [w_min, w_max]
* [ ] `weighted_photometric_loss(pred_rgb, gt_rgb, flow_map) -> Tensor`
  * 组合：
    * `flow_magnitude` → `build_dynamic_weight_map`
    * 用 weight 对 L1+SSIM 做加权平均

### 5.4 总损失封装

* [ ] `compute_total_loss(pred_rgb, gt_rgb, pred_depth, gt_depth, velocities, flow_map, loss_cfg) -> Dict[str, Tensor]`
  * 计算：
    * `L_rgb`（带动态权重）
    * `L_depth`
    * `L_vel`
  * 根据 config 加权求和
  * 返回：
    * `L_total`
    * 各项子损失（便于日志记录）

---

## 模块六：训练引擎（参考 `engine_storm.py` 设计 `engine_flux4d.py`）

### 6.1 数据打包与前向

 **文件** ：`engine_flux4d.py`

* [ ] `prepare_batch(batch_items) -> (G_init, cam_params, target_images, target_depths, times)`
  * 从 DataLoader 拿到的原始 batch 结构中：
    * 加载 `G_init`（或在线调用 `lift_lidar_frame_to_gaussians`）
    * 整理摄像机参数、目标图像、深度、时间戳等
* [ ] `forward_model(model, renderer, batch_struct) -> Dict[str, Any]`
  * 调用：
    * `model.forward(G_init) -> G_refined, V`
    * 在一个/多个时间点上调用 `render_rgb_depth`
    * 计算光流 `render_flow_map`
  * 返回：
    * `pred_rgb`, `pred_depth`, `pred_flow`, `velocities` 等

### 6.2 损失 + 反向

* [ ] `compute_losses(loss_module, preds, gts, cfg) -> Dict[str, Tensor]`
  * 使用 `compute_total_loss`
* [ ] `train_step(model, renderer, optimizer, batch, loss_module, scaler=None) -> Dict[str, float]`
  * 完整训练步骤：
    * `forward_model`
    * `compute_losses`
    * 反向 & 更新（支持 AMP）
* [ ] `val_step(model, renderer, batch, loss_module) -> Dict[str, float]`
  * 只做前向与 loss/metric，便于验证

### 6.3 训练循环与多 GPU

* [ ] `train_epoch(train_loader, model, renderer, optimizer, loss_module, logger, epoch, cfg) -> Dict[str, float]`
  * 遍历 train_loader，聚合平均 loss
* [ ] `validate_epoch(val_loader, model, renderer, loss_module, logger, epoch, cfg) -> Dict[str, float]`
* [ ] `save_checkpoint(model, optimizer, epoch, path)`
* [ ] `load_checkpoint(path) -> (model_state, optimizer_state, epoch)`
* [ ] `setup_distributed_from_main(args) -> (rank, world_size)`
  * 对齐 GaussianSTORM 的 `torchrun --nproc_per_node=...` 用法

---

## 模块七：训练/推理脚本（参考 GaussianSTORM 的 `main_storm.py` / `inference.py`）

### 7.1 训练入口：`main_flux4d.py`

* [ ] `parse_train_args() -> argparse.Namespace`
  * 保持与 `main_storm.py` 类似参数风格：
    * `--project, --exp_name, --data_root, --batch_size, --num_iterations, --model, ...`
    * 增加 Flux4D 特有参数：`--lambda_vel`, `--dynamic_weight_alpha` 等
* [ ] `build_model_and_renderer_from_args(args) -> (model, renderer)`
* [ ] `build_dataloaders_from_args(args) -> (train_loader, val_loader)`
* [ ] `main()`
  * 组装所有组件，进入 training loop

### 7.2 推理入口：`inference_flux4d.py`

* [ ] `parse_infer_args()`
  * 与 GaussianSTORM 的 `inference.py` 类似：
    * `--project, --exp_name, --data_root, --model, --load_from, ...`
* [ ] `load_model_from_checkpoint(args) -> (model, renderer)`
* [ ] `run_inference_on_clip(model, renderer, clip_meta, save_dir)`
  * 在多时间点、多视角渲染：
    * RGB / Depth / Flow
  * 保存为图像或视频
* [ ] `main()`
  * 支持对单个/多个 clip 做可视化

---

这份清单已经：

* 显式对齐了 GaussianSTORM 的工程结构（`main_storm.py / engine_storm.py / preprocess.py / inference.py / storm/`；本项目对应 `models/`）
* 又完整融入你要复现的 Flux4D 关键点：
  * LiDAR→高斯的 Lift 模块
  * 变长高斯 → 稀疏体素 → Transformer/Perceiver
  * 对每个高斯预测 3D 速度 + 参数更新
  * 时间推进 + 3DGS 渲染
  * 速度正则 + 基于光流的动态区域重加权

你可以直接按这个函数级 TODO 去实现，优先顺序建议：

1. 数据预处理 + Lift（模块一、二）
2. 渲染器前向打通（模块四）
3. 简化版模型（先只预测静态高斯，不含速度）+ 静态 photometric loss
4. 再加入速度分支 + 线性运动模型 + 速度正则
5. 最后加上光流渲染与动态区域重加权，做完整 Flux4D 训练。

# 需求1-codex

目标：复现flux4d论文的代码，达到论文的指标。

数据路径：

1. pandaset数据：/home/yr/yr/data/automonous/pandaset
2. waymo数据：暂时未下载

先以pandaset数据来复现，a. 先生成完整的数据标签，b. 同时要生成一个小的数据标签，比如只提取一个场景或者两个场景数据的标签，在代码调试期间，使用这个小数据来调试代码

相关代码重点参考/home/yr/yr/code/cv/AutoLabel/SSL/GaussianSTORM_all/GaussianSTORM中实现的代码，当前网上有其它好的参考，你也需要考虑。GaussianSTORM的运行环境是/home/yr/anaconda3/envs/storm。

关键的模块需要进行校验，比如数据处理，必须有可视化的校验，确保正常。

如果对应模块网上有相关成熟的实现，优先使用成熟的库和代码，不要重复造轮子。

初版计划-MiroThinker和顶层结构建议-MiroThinker的内容，是目前给定的初版实现方案

你需要对上面的初版计划-MiroThinker和顶层结构建议-MiroThinker的内容进行评审，给出你的建议。然后给出你认为最好的实现方案，将你的方案写在本文档的方案制定1-codex标题中

# 方案制定1-codex

## 评审与建议

### 初版计划-MiroThinker

- 优点
  - 模块拆分清晰（数据、Lift、Predict、Render、Loss、Engine），便于并行推进与分工。
  - 关键技术点覆盖完整（3DGS、稀疏卷积、时间推进、速度正则、动态重加权）。
  - 阶段化路线合理，先静态后动态，符合工程落地节奏。
- 需要补充与修正
  - “数据标签”的定义要前置，明确为可复现的 clip 索引 + 可选缓存，避免接口反复变化。
  - 关键模块必须有可视化门禁，尤其是相机-LiDAR 对齐、时间同步、坐标系方向。
  - 依赖栈要尽早锁定（渲染后端、稀疏卷积库、SSIM/flow 工具），避免双栈维护。
  - 与 GaussianSTORM 的复用点需具体化（dataset/engine/logger/config/renderer）。
  - 评测协议需提前定下（PSNR/SSIM、Depth RMSE、SceneFlow/Flow EPE、速度统计）。

### 顶层结构建议-MiroThinker

- 优点
  - 顶层结构与 GaussianSTORM 相似，迁移成本低。
  - Lift/render/losses 分层清晰。
- 调整建议
  - 增加 `data/metadata/` 与 `data/cache/`，专门存放 clip 索引与 Lift 缓存。
  - 增加 `tools/vis/`（或 `scripts/vis/`）承载所有可视化校验脚本。
  - 增加 `configs/`（或复用 GaussianSTORM 的配置风格）用于可复现实验。
  - 明确“离线 Lift 缓存”和“在线 Lift”两条路径，训练默认走离线缓存。

## 最佳实现方案（v1）

### 0. 目标与约束

- 先完成 PandaSet（路径：`/home/yr/yr/data/automonous/pandaset`），Waymo 后续扩展。
- 需要 full/tiny 两套“数据标签”用于训练与调试。
- 关键模块必须可视化校验通过才进入下一阶段。
- 能复用就复用，避免重复造轮子。

### 1. 依赖锁定与参考基线

1. PandaSet 读写：`pandaset-devkit` + OpenCV/Open3D。
2. 渲染后端：沿用 GaussianSTORM 已集成的 gsplat/gaussian-splatting（统一接口）。
3. 稀疏卷积：仅选一种（优先与 GaussianSTORM 一致；若不可用再换 MinkowskiEngine）。
4. 损失/评测：SSIM 用 kornia 或等价实现；flow 可视化用成熟工具库。
5. 参考代码：`/home/yr/yr/code/cv/AutoLabel/SSL/GaussianSTORM_all/GaussianSTORM` 作为工程骨架。

### 2. 数据标签方案（full + tiny）

**定义**：数据标签 = 可复现的 clip 索引元数据 + 可选 Lift 缓存。

- 输出文件
  - `data/metadata/pandaset_full_clips.pkl`
  - `data/metadata/pandaset_tiny_clips.pkl`
- 每条 clip 记录建议字段
  - `dataset`, `sequence_id`, `clip_id`, `frame_ids`, `timestamps`
  - `cams`: 每帧多相机图像路径 + 内外参
  - `lidar`: 路径 + 外参
  - `world_frame`: 统一坐标系定义与时间基准
- tiny 生成策略
  - 固定场景 ID 列表 + 固定随机种子（保证可复现）
  - 同时生成 1.5s 与 8s 两种 clip，覆盖短/长时序逻辑
- 可选缓存
  - `data/cache/pandaset_full/`、`data/cache/pandaset_tiny/` 保存 Lift 结果（.pt/.npz）

### 3. 关键模块可视化门禁（必须落地）

1. 数据对齐（门禁 1）
   - LiDAR 点投影叠加到相机图像，检查外参/时间对齐。
   - 俯视/侧视点云可视化，确认坐标系方向与尺度。
2. Lift（门禁 2）
   - 初始高斯渲染到图像，与原图对比。
   - 高斯颜色与投影取色一致性检查。
3. 渲染（门禁 3）
   - 静态渲染 RGB/Depth 与 GT 对比。
   - 光流渲染可视化，检查动态区域权重是否合理。
4. 训练（门禁 4）
   - tiny 上过拟合（单 clip 快速收敛）。
   - 速度分布统计：静态背景速度接近 0。

### 4. 工程结构建议（对齐 GaussianSTORM）

- `configs/`：实验配置
- `data/metadata/`：clip 索引
- `data/cache/`：Lift 缓存
- `preprocess_flux4d.py`：索引与标签生成
- `lift/`：LiDAR -> 高斯
- `render/`：时间推进 + 3DGS 渲染 + 光流渲染
- `losses/`：光度损失/速度正则/动态重加权
- `engine_flux4d.py`、`main_flux4d.py`、`inference_flux4d.py`
- `tools/vis/`：投影、深度、光流、速度统计可视化

### 5. 实施里程碑（从小到大）

阶段 0：环境与基线验证（1 周）

- 跑通 GaussianSTORM 训练与渲染脚手架。
- 完成 LiDAR 投影到图像的可视化门禁。

阶段 1：数据标签与 Lift（1-2 周）

- 生成 full/tiny clip 索引（含时间与坐标系定义）。
- 完成 LiDAR -> 高斯，并输出可视化验证结果。
- 产出 Lift 缓存与读取接口。

阶段 2：静态 Flux4D（2-3 周）

- 接入稀疏体素特征与轻量 head，仅预测高斯更新。
- tiny 上过拟合成功，RGB/Depth loss 明显下降。

阶段 3：速度与时间推进（3-4 周）

- 预测速度向量，加入线性时间推进渲染。
- 速度正则收敛，静态背景速度接近 0。

阶段 4：光流与动态重加权（2-3 周）

- 渲染光流并加入动态权重。
- tiny 上动态区域更清晰、运动稳定。

阶段 5：全量训练与评测（持续迭代）

- full 数据训练与评测，对齐论文指标与趋势。

### 6. 评测与验收标准

- tiny 上可视化无明显错位，过拟合成功。
- 静态背景速度均值接近 0，动态区域速度非零。
- PSNR/SSIM、Depth RMSE、Flow/SceneFlow EPE 与论文同量级。

### 7. 风险与对策

- 数据对齐误差：投影可视化作为硬门禁，不通过不进入训练。
- 规模过大：先下采样与分块训练，逐步放大。
- 依赖冲突：先锁定渲染与稀疏卷积后再扩展功能。

# 顶层结构建议-codex

## 设计原则

- 根目录保持简洁，核心代码集中在 `src/`。
- 数据集不入库，仅保存可复现的索引与缓存。
- 与 GaussianSTORM 的组织习惯对齐，方便复用与迁移。
- 可视化校验脚本与训练脚本分离，便于门禁验证。

## 建议结构（v1）

```
.
├── configs/                 # 训练/评测配置
├── data/
│   ├── metadata/            # full/tiny clip 索引
│   └── cache/               # Lift 缓存（可选）
├── src/
│   └── flux4d/
│       ├── datasets/        # PandaSet/Waymo 读取与对齐
│       ├── lift/            # LiDAR -> Gaussians
│       ├── render/          # 时间推进 + 3DGS 渲染 + 光流
│       ├── models/          # 核心模型（复用 GaussianSTORM 思路）
│       ├── losses/          # 光度/速度/动态重加权
│       ├── metrics/         # PSNR/SSIM/Depth/Flow 指标
│       ├── engine/          # 训练与评测循环
│       └── utils/           # 通用工具与数值辅助
├── scripts/
│   ├── preprocess_flux4d.py # 生成 full/tiny 索引
│   ├── train_flux4d.py      # 训练入口
│   └── infer_flux4d.py      # 推理与导出
├── tools/
│   └── vis/                 # 投影/深度/光流/速度可视化
├── tests/                   # 关键模块单测（后续补齐）
├── docs/                    # 文档与实验记录
├── assets/                  # 图表与可视化产物（可选）
└── third_party/             # 外部依赖仓库（如 gsplat/3DGS）
```

## 关键约定

- `data/` 仅放索引与缓存，真实数据仍在 `/home/yr/yr/data/automonous/pandaset`。
- `scripts/` 只做入口编排，核心逻辑留在 `src/flux4d/`。
- `third_party/` 仅用于必要依赖，避免散落在根目录。

# 实现计划-codex

## 阶段 0：环境与基线确认（0.5-1 周）

- 锁定渲染后端与稀疏卷积库（优先与 GaussianSTORM 一致）。
- 跑通 GaussianSTORM 的训练/渲染脚手架，确认依赖可用。
- 输出：基础环境说明、配置模板、依赖版本列表。

## 阶段 1：数据标签与可视化门禁（1-2 周）

- 实现 `scripts/preprocess_flux4d.py`，生成 full/tiny clip 索引。
- 定义并固化 clip 字段（时间、坐标系、相机/雷达外参）。
- 完成数据对齐可视化：
  - LiDAR 点投影叠加图（多相机）。
  - 点云俯视/侧视图检查坐标系。
- 输出：
  - `data/metadata/pandaset_full_clips.pkl`
  - `data/metadata/pandaset_tiny_clips.pkl`
  - `tools/vis/` 下的对齐示例图

## 阶段 2：Lift 与缓存（1-2 周）

- 实现 `src/flux4d/lift/`：
  - LiDAR 体素下采样与 kNN 尺度估计。
  - 多相机投影取色与高斯初始化。
- 增加可选缓存写入与读取接口。
- 完成 Lift 可视化门禁：
  - 初始高斯渲染图与原图对比。
  - 颜色采样一致性检查。
- 输出：
  - `data/cache/pandaset_tiny/` 样例
  - Lift 可视化结果

## 阶段 3：渲染与光流（1-2 周）

- 实现 `src/flux4d/render/`：
  - 线性时间推进位置更新。
  - RGB/Depth 渲染。
  - 光流渲染与可视化。
- 渲染门禁：
  - 静态渲染对比 GT。
  - 光流图检查动态区域。

## 阶段 4：静态模型基线（2-3 周）

- 实现最小可用模型（不含速度）：
  - 稀疏体素编码 + 轻量 head。
- tiny 数据过拟合验证（loss 明显下降）。
- 输出：
  - 训练日志与可视化对比图。

## 阶段 5：速度分支与时间建模（2-3 周）

- 加入速度预测与线性时间推进渲染。
- 引入速度正则，统计速度分布。
- 门禁：
  - 静态背景速度接近 0。
  - 动态目标速度非零。

## 阶段 6：动态重加权（1-2 周）

- 加入基于光流的动态权重机制。
- tiny 上验证动态区域更清晰，运动稳定。

## 阶段 7：全量训练与评测（持续迭代）

- full 数据训练与评测，对齐论文指标。
- 输出指标：
  - PSNR/SSIM、Depth RMSE、Flow/SceneFlow EPE。
- 记录误差案例与可视化，推动迭代。

## 里程碑交付物汇总

- 索引与缓存：`data/metadata/`、`data/cache/`
- 可视化门禁结果：`tools/vis/`
- 训练/推理入口：`scripts/train_flux4d.py`、`scripts/infer_flux4d.py`
- 配置与实验记录：`configs/`、`docs/`
